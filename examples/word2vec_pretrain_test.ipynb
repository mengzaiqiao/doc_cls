{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load BioCaser Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parse biocaser data from /home/zm324/workspace/doc_cls/data/biocaster/BioCaster.3.xml, docs number:1003, lablels number:1003\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docs</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\nBird Flu Outbreak Drill Spooks Manitoba Town...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\nTyphoid outbreak in Agusan del Sur town unde...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n  Typhoid Outbreak In Central Nepal November...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\nChlorine truck crash shuts Turnpike exit\\nPo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\nMan dies from rare anthrax bug \\nChristopher...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                docs  labels\n",
       "0  \\nBird Flu Outbreak Drill Spooks Manitoba Town...       0\n",
       "1  \\nTyphoid outbreak in Agusan del Sur town unde...       1\n",
       "2  \\n  Typhoid Outbreak In Central Nepal November...       1\n",
       "3  \\nChlorine truck crash shuts Turnpike exit\\nPo...       0\n",
       "4  \\nMan dies from rare anthrax bug \\nChristopher...       1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.data_util import biocaser2text\n",
    "data_file = \"/home/zm324/workspace/doc_cls/data/biocaster/BioCaster.3.xml\"\n",
    "data_df = biocaser2text(data_file)\n",
    "map_dic = {\"negative\": 0, \"positive\": 1}\n",
    "data_df.labels = data_df.labels.apply(lambda x: map_dic[x])\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get promed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 3009 files\n",
      "found files: 3009\n",
      "found 1530 files\n",
      "found files: 1530\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docs</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n \\n \\n \\n AVIAN INFLUENZA, HUMAN (109) - IND...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Health officials in Hong Kong say that...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n \\n EQUINE INFLUENZA - CHINA\\n \\n **********...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\n \\n \\n \\n SALMONELLOSIS SENFTENBERG, BASIL -...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n \\n GIARDIASIS, HUMAN - UNITED KINGDOM: (YOR...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                docs  labels\n",
       "0  \\n \\n \\n \\n AVIAN INFLUENZA, HUMAN (109) - IND...       1\n",
       "1          Health officials in Hong Kong say that...       1\n",
       "2  \\n \\n EQUINE INFLUENZA - CHINA\\n \\n **********...       1\n",
       "3  \\n \\n \\n \\n SALMONELLOSIS SENFTENBERG, BASIL -...       1\n",
       "4  \\n \\n GIARDIASIS, HUMAN - UNITED KINGDOM: (YOR...       1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.data_util import (\n",
    "    biocaser2text,\n",
    "    get_raw_extended_promed_df,\n",
    "    get_raw_promed_df,\n",
    ")\n",
    "data_df = get_raw_promed_df()\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get promed-extended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 3430 files\n",
      "Remain 3377 files after filter_out with alerting\n",
      "found files: 3377\n",
      "found 3430 files\n",
      "Remain 53 files after filter_in with alerting\n",
      "found files: 53\n",
      "found 3862 files\n",
      "found files: 3862\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docs</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n \\n \\n \\n AVIAN INFLUENZA, HUMAN (109) - IND...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Health officials in Hong Kong say that...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;h&gt; Coronavirus Detected In Patient Quarantine...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\n \\n EQUINE INFLUENZA - CHINA\\n \\n **********...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n \\n \\n \\n SALMONELLOSIS SENFTENBERG, BASIL -...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                docs  labels\n",
       "0  \\n \\n \\n \\n AVIAN INFLUENZA, HUMAN (109) - IND...       1\n",
       "1          Health officials in Hong Kong say that...       1\n",
       "2  <h> Coronavirus Detected In Patient Quarantine...       1\n",
       "3  \\n \\n EQUINE INFLUENZA - CHINA\\n \\n **********...       1\n",
       "4  \\n \\n \\n \\n SALMONELLOSIS SENFTENBERG, BASIL -...       1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.data_util import (\n",
    "    biocaser2text,\n",
    "    get_raw_extended_promed_df,\n",
    "    get_raw_promed_df,\n",
    ")\n",
    "data_df = get_raw_extended_promed_df()\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-4e5bbc2ba586>:2: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  word2vec = model.wv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# model = gensim.models.KeyedVectors.load_word2vec_format('/home/zm324/workspace/doc_cls/resources/word2vec/GoogleNews-vectors-negative300.bin.gz', binary=True) \n",
    "# word2vec = model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EmbeddingBag(1, 300, mode=mean)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import torch\n",
    "# import gensim\n",
    "# import time\n",
    "# import numpy as np\n",
    "# from tqdm import tqdm\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# from torch.utils.data import DataLoader\n",
    "# embedding = nn.EmbeddingBag(1, 300, sparse=True)\n",
    "# embedding.from_pretrained(torch.tensor([word2vec[\"word\"]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gensim\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class TextLinear(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class,emb_pretrain):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "        self.pre_train = emb_pretrain\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.from_pretrained(self.pre_train)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        return self.fc(embedded)\n",
    "def generate_batch_wo_label(batch):\n",
    "    text = [torch.tensor(entry) for entry in batch]\n",
    "    offsets = [0] + [len(entry) for entry in text]\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text = torch.cat(text)\n",
    "    return text, offsets\n",
    "\n",
    "def generate_batch(batch):\n",
    "    label = torch.tensor([entry[0] for entry in batch])\n",
    "    text = [torch.tensor(entry[1]) for entry in batch]\n",
    "    offsets = [0] + [len(entry) for entry in text]\n",
    "    # torch.Tensor.cumsum returns the cumulative sum\n",
    "    # of elements in the dimension dim.\n",
    "    # torch.Tensor([1.0, 2.0, 3.0]).cumsum(dim=0)\n",
    "\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text = torch.cat(text)\n",
    "    return text, offsets, label\n",
    "    \n",
    "class RandomWord2vec(object):\n",
    "    \n",
    "    def __init__(self, data_df, emb_dim=300, batch_size=128, max_epochs = 50, num_class = 2):\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else \"cpu\"\n",
    "        from utils.preprocess import PreProcess\n",
    "        pre_processor = PreProcess(data_df, \"docs\",lower=False)\n",
    "        # todo: change code to provide all functions in class definition.\n",
    "        pre_processor.clean_html()\n",
    "        pre_processor.remove_non_ascii()\n",
    "        pre_processor.remove_spaces()\n",
    "        pre_processor.remove_punctuation()\n",
    "        pre_processor.stop_words()\n",
    "        # pre_processor.tokenize()\n",
    "        data_df.head()\n",
    "        self.data_df = data_df\n",
    "        self.emb_dim = emb_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.max_epochs = max_epochs\n",
    "        self.num_class = num_class\n",
    "        self.word2vec = gensim.models.KeyedVectors.load_word2vec_format('/home/zm324/workspace/doc_cls/resources/word2vec/GoogleNews-vectors-negative300.bin.gz', binary=True).wv\n",
    "        self.build_vocab()\n",
    "        self.pre_train = torch.tensor(self.pre_train).to(self.device)\n",
    "        self.data_df.docs=self.data_df.docs.apply(self.doc2idx)\n",
    "        \n",
    "    def doc2idx(self, doc):\n",
    "        idxs = []\n",
    "        if type(doc) is not list:\n",
    "            doc = doc.split()\n",
    "        for word in doc:\n",
    "            if word in self.word_to_ix:\n",
    "                idxs.append(self.word_to_ix[word])\n",
    "        return idxs\n",
    "        \n",
    "    def build_vocab(self):\n",
    "        self.pre_train = []\n",
    "        word_to_ix = {}\n",
    "        for idx,row in self.data_df.iterrows():\n",
    "            doc = row.docs\n",
    "            if type(doc) is not list:\n",
    "                doc = doc.split()\n",
    "            for word in doc:\n",
    "                if word not in word_to_ix and word in self.word2vec:\n",
    "                    word_to_ix[word] = len(word_to_ix)\n",
    "                    self.pre_train.append(self.word2vec[word])\n",
    "        self.word_to_ix = word_to_ix\n",
    "        self.vocab_size = len(self.word_to_ix)\n",
    "    \n",
    "    \n",
    "    def train_epoch(self,X,y):\n",
    "\n",
    "        # Train the model\n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        tran_data = np.stack((y,X), axis=-1)\n",
    "        data = DataLoader(tran_data, batch_size=self.batch_size, shuffle=True,\n",
    "                          collate_fn=generate_batch)\n",
    "        for i, (text, offsets, cls) in enumerate(data):\n",
    "            self.optimizer.zero_grad()\n",
    "            text, offsets, cls = text.to(self.device), offsets.to(self.device), cls.to(self.device)\n",
    "            output = self.model(text, offsets)\n",
    "            loss = self.criterion(output, cls)\n",
    "            train_loss += loss.item()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            train_acc += (output.argmax(1) == cls).sum().item()\n",
    "\n",
    "        # Adjust the learning rate\n",
    "        self.scheduler.step()\n",
    "\n",
    "        return train_loss / len(X), train_acc / len(X)\n",
    "    \n",
    "    def fit(self,X,y):\n",
    "        self.model = TextLinear(self.vocab_size, self.emb_dim, self.num_class, self.pre_train).to(self.device)\n",
    "        min_valid_loss = float('inf')\n",
    "        self.criterion = torch.nn.CrossEntropyLoss().to(self.device)\n",
    "        self.optimizer = torch.optim.SGD(self.model.parameters(), lr=4.0)\n",
    "        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, 1, gamma=0.9)\n",
    "\n",
    "        for epoch in tqdm(range(self.max_epochs)):\n",
    "            start_time = time.time()\n",
    "            train_loss, train_acc = self.train_epoch(X,y)\n",
    "            secs = int(time.time() - start_time)\n",
    "            mins = secs / 60\n",
    "            secs = secs % 60\n",
    "\n",
    "#             print('Epoch: %d' %(epoch + 1), \" | time in %d minutes, %d seconds\" %(mins, secs))\n",
    "#             print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.1f}%(train)')\n",
    "\n",
    "    def predict(self,X):\n",
    "        loss = 0\n",
    "        acc = 0\n",
    "        data = DataLoader(X, batch_size=self.batch_size, collate_fn=generate_batch_wo_label)\n",
    "        pred_y = []\n",
    "        for text, offsets in data:\n",
    "            text, offsets = text.to(self.device), offsets.to(self.device)\n",
    "            with torch.no_grad():\n",
    "                output = self.model(text, offsets).to('cpu')\n",
    "                pred_y+=list(output.argmax(1))\n",
    "        return pred_y\n",
    "    #     return loss / len(data_), acc / len(data_)\n",
    "\n",
    "    def test(self,data_):\n",
    "        loss = 0\n",
    "        acc = 0\n",
    "        data = DataLoader(data_, batch_size=self.batch_size, collate_fn=generate_batch)\n",
    "        pred_y = []\n",
    "        for text, offsets, cls in data:\n",
    "            text, offsets, cls = text.to(self.device), offsets.to(self.device), cls.to(self.device)\n",
    "            with torch.no_grad():\n",
    "                output = self.model(text, offsets)\n",
    "                loss = criterion(output, cls)\n",
    "                loss += loss.item()\n",
    "                pred_y = output.argmax(1)\n",
    "                acc += (output.argmax(1) == cls).sum().item()\n",
    "    #     return pred_y\n",
    "        return loss / len(data_), acc / len(data_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-14-89cd6cfb6318>:65: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  self.word2vec = gensim.models.KeyedVectors.load_word2vec_format('/home/zm324/workspace/doc_cls/resources/word2vec/GoogleNews-vectors-negative300.bin.gz', binary=True).wv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docs</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 12, 8...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[130, 131, 132, 133, 134, 135, 73, 136, 137, 1...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[183, 184, 185, 186, 187, 188, 189, 190, 191, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[237, 238, 239, 240, 241, 242, 243, 12, 244, 2...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>[1600, 446, 4659, 10819, 244, 245, 729, 4360, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>[130, 723, 1188, 23853, 2395, 23854, 3972, 12,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>[23865, 2967, 2693, 73, 1538, 7423, 389, 15576...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>[723, 8443, 73, 602, 23641, 1001, 23869, 1325,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>[5254, 768, 2575, 73, 141, 702, 244, 4587, 146...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1003 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   docs  labels\n",
       "0     [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...       0\n",
       "1     [70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 12, 8...       1\n",
       "2     [130, 131, 132, 133, 134, 135, 73, 136, 137, 1...       1\n",
       "3     [183, 184, 185, 186, 187, 188, 189, 190, 191, ...       0\n",
       "4     [237, 238, 239, 240, 241, 242, 243, 12, 244, 2...       1\n",
       "...                                                 ...     ...\n",
       "998   [1600, 446, 4659, 10819, 244, 245, 729, 4360, ...       1\n",
       "999   [130, 723, 1188, 23853, 2395, 23854, 3972, 12,...       1\n",
       "1000  [23865, 2967, 2693, 73, 1538, 7423, 389, 15576...       1\n",
       "1001  [723, 8443, 73, 602, 23641, 1001, 23869, 1325,...       1\n",
       "1002  [5254, 768, 2575, 73, 141, 702, 244, 4587, 146...       1\n",
       "\n",
       "[1003 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RandomWord2vec(data_df)\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]/home/zm324/anaconda3/envs/doc_cls/lib/python3.8/site-packages/numpy/core/_asarray.py:136: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order, subok=True)\n",
      "100%|██████████| 50/50 [00:01<00:00, 46.52it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 45.10it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 42.63it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 42.99it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 43.04it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 43.04it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 42.77it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 42.80it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 42.72it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 43.35it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 42.94it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 42.57it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 43.14it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 42.75it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 42.74it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 43.17it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 42.77it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 42.92it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 42.87it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 42.48it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 42.76it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 43.31it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 42.92it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 42.29it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 43.87it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 42.37it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 42.80it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 42.47it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 43.28it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 42.56it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 42.77it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 42.79it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 42.85it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 42.94it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 42.62it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 42.85it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 42.84it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 43.46it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 45.18it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 46.20it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 47.34it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 47.47it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 47.61it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 47.33it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 47.53it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 47.31it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 47.52it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 47.38it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 46.86it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 47.59it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 47.80it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 47.20it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 47.46it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 47.22it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 47.46it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 47.47it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 47.24it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 47.54it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 47.77it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 47.37it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 47.27it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 47.14it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 47.40it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 47.44it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 47.48it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 47.61it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 46.87it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 47.12it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 48.00it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 47.37it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 47.55it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 47.17it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 47.57it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 47.07it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 47.70it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 47.24it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 47.28it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 47.16it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 47.74it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 47.98it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 47.57it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 47.60it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 47.53it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 47.42it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 46.92it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 47.67it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 47.11it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 47.68it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 47.75it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 47.43it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 47.22it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 47.46it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 47.43it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 47.34it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 47.22it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 47.22it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 47.58it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 47.48it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 47.23it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 47.76it/s]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RepeatedKFold\n",
    "from metrics import Accuracy,Precision,Recall,F1Score\n",
    "metrics = [Accuracy(),Precision(),Recall(),F1Score()]\n",
    "\n",
    "avg_results = {m.name:[] for m in metrics}\n",
    "rkf = RepeatedKFold(n_splits=10, n_repeats=10, random_state=2652124)\n",
    "for train_index, test_index in rkf.split(data_df):\n",
    "    train_set = data_df.iloc[train_index]\n",
    "    test_set = data_df.iloc[test_index]\n",
    "    X = list(train_set[\"docs\"])\n",
    "    y_true = list(train_set[\"labels\"])\n",
    "    model.fit(X,y_true)\n",
    "\n",
    "    X = list(test_set[\"docs\"])\n",
    "    y_true = list(test_set[\"labels\"])\n",
    "    y_pred = model.predict(X)\n",
    "    for mt in metrics:\n",
    "        avg_results[mt.name].append(mt.compute(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mt in metrics:\n",
    "    avg_results[mt.name]=np.mean(avg_results[mt.name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8656940594059406,\n",
       " 'precision': 0.8317282294923942,\n",
       " 'recall': 0.8041507476094318,\n",
       " 'f1_score': 0.8153097206248237}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "doc_cls",
   "language": "python",
   "name": "doc_cls"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
