{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "120000lines [00:04, 28507.26lines/s]\n",
      "120000lines [00:08, 14365.43lines/s]\n",
      "7600lines [00:00, 14912.63lines/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torchtext.datasets import text_classification\n",
    "\n",
    "\n",
    "NGRAMS = 2\n",
    "import os\n",
    "if not os.path.isdir('../data'):\n",
    "    os.mkdir('../data')\n",
    "train_dataset, test_dataset = text_classification.DATASETS['AG_NEWS'](\n",
    "    root='../data', ngrams=NGRAMS, vocab=None)\n",
    "BATCH_SIZE = 16\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parse biocaser data from ../data/biocaster/BioCaster.3.xml, docs number:1003, lablels number:1003\n"
     ]
    }
   ],
   "source": [
    "from data_util import biocaser2text\n",
    "data_file = \"../data/biocaster/BioCaster.3.xml\"\n",
    "docs, labels = biocaser2text(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nBird Flu Outbreak Drill Spooks Manitoba Town  \\nJason McIntyre Saturday, November 4, 2006   \\nThere are no moon men in Steinbach, Manitoba.  Local residents phoned and e-mailed radio stations as far away as Winnipeg wanting to know why people in eerie space suits had blocked off a road and were working at a community farm.  According to the Canadian Food Inspection Agency spokesperson Sandra Stephens, there is no need for alarm. Those participating in the mock avian influenza drill had biohazard suits and cordoned off a specific area.  Federal Agriculture Minister Chuck Strahl said the event wasnt publicized to ensure the test parameters were as close to real-world as possible.      \\n\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class TextSentiment(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        return self.fc(embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(train_dataset.get_vocab())\n",
    "EMBED_DIM = 32\n",
    "NUN_CLASS = len(train_dataset.get_labels())\n",
    "model = TextSentiment(VOCAB_SIZE, EMBED_DIM, NUN_CLASS).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(batch):\n",
    "    label = torch.tensor([entry[0] for entry in batch])\n",
    "    text = [entry[1] for entry in batch]\n",
    "    offsets = [0] + [len(entry) for entry in text]\n",
    "    # torch.Tensor.cumsum returns the cumulative sum\n",
    "    # of elements in the dimension dim.\n",
    "    # torch.Tensor([1.0, 2.0, 3.0]).cumsum(dim=0)\n",
    "\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text = torch.cat(text)\n",
    "    return text, offsets, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train_func(sub_train_):\n",
    "\n",
    "    # Train the model\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    data = DataLoader(sub_train_, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                      collate_fn=generate_batch)\n",
    "    for i, (text, offsets, cls) in enumerate(data):\n",
    "        optimizer.zero_grad()\n",
    "        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n",
    "        output = model(text, offsets)\n",
    "        loss = criterion(output, cls)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_acc += (output.argmax(1) == cls).sum().item()\n",
    "\n",
    "    # Adjust the learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    return train_loss / len(sub_train_), train_acc / len(sub_train_)\n",
    "\n",
    "def test(data_):\n",
    "    loss = 0\n",
    "    acc = 0\n",
    "    data = DataLoader(data_, batch_size=BATCH_SIZE, collate_fn=generate_batch)\n",
    "    for text, offsets, cls in data:\n",
    "        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(text, offsets)\n",
    "            loss = criterion(output, cls)\n",
    "            loss += loss.item()\n",
    "            acc += (output.argmax(1) == cls).sum().item()\n",
    "\n",
    "    return loss / len(data_), acc / len(data_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1  | time in 0 minutes, 4 seconds\n",
      "\tLoss: 0.0041(train)\t|\tAcc: 98.1%(train)\n",
      "\tLoss: 0.0000(valid)\t|\tAcc: 96.7%(valid)\n",
      "Epoch: 2  | time in 0 minutes, 4 seconds\n",
      "\tLoss: 0.0023(train)\t|\tAcc: 99.0%(train)\n",
      "\tLoss: 0.0000(valid)\t|\tAcc: 98.3%(valid)\n",
      "Epoch: 3  | time in 0 minutes, 4 seconds\n",
      "\tLoss: 0.0014(train)\t|\tAcc: 99.5%(train)\n",
      "\tLoss: 0.0000(valid)\t|\tAcc: 98.2%(valid)\n",
      "Epoch: 4  | time in 0 minutes, 4 seconds\n",
      "\tLoss: 0.0010(train)\t|\tAcc: 99.6%(train)\n",
      "\tLoss: 0.0000(valid)\t|\tAcc: 98.3%(valid)\n",
      "Epoch: 5  | time in 0 minutes, 4 seconds\n",
      "\tLoss: 0.0007(train)\t|\tAcc: 99.7%(train)\n",
      "\tLoss: 0.0000(valid)\t|\tAcc: 98.3%(valid)\n",
      "Epoch: 6  | time in 0 minutes, 4 seconds\n",
      "\tLoss: 0.0006(train)\t|\tAcc: 99.8%(train)\n",
      "\tLoss: 0.0000(valid)\t|\tAcc: 98.2%(valid)\n",
      "Epoch: 7  | time in 0 minutes, 4 seconds\n",
      "\tLoss: 0.0004(train)\t|\tAcc: 99.8%(train)\n",
      "\tLoss: 0.0000(valid)\t|\tAcc: 98.0%(valid)\n",
      "Epoch: 8  | time in 0 minutes, 4 seconds\n",
      "\tLoss: 0.0003(train)\t|\tAcc: 99.9%(train)\n",
      "\tLoss: 0.0000(valid)\t|\tAcc: 98.3%(valid)\n",
      "Epoch: 9  | time in 0 minutes, 4 seconds\n",
      "\tLoss: 0.0003(train)\t|\tAcc: 99.9%(train)\n",
      "\tLoss: 0.0000(valid)\t|\tAcc: 98.3%(valid)\n",
      "Epoch: 10  | time in 0 minutes, 4 seconds\n",
      "\tLoss: 0.0002(train)\t|\tAcc: 99.9%(train)\n",
      "\tLoss: 0.0000(valid)\t|\tAcc: 98.3%(valid)\n",
      "Epoch: 11  | time in 0 minutes, 4 seconds\n",
      "\tLoss: 0.0002(train)\t|\tAcc: 99.9%(train)\n",
      "\tLoss: 0.0000(valid)\t|\tAcc: 98.2%(valid)\n",
      "Epoch: 12  | time in 0 minutes, 4 seconds\n",
      "\tLoss: 0.0002(train)\t|\tAcc: 99.9%(train)\n",
      "\tLoss: 0.0000(valid)\t|\tAcc: 98.2%(valid)\n",
      "Epoch: 13  | time in 0 minutes, 4 seconds\n",
      "\tLoss: 0.0002(train)\t|\tAcc: 99.9%(train)\n",
      "\tLoss: 0.0000(valid)\t|\tAcc: 98.3%(valid)\n",
      "Epoch: 14  | time in 0 minutes, 4 seconds\n",
      "\tLoss: 0.0002(train)\t|\tAcc: 100.0%(train)\n",
      "\tLoss: 0.0000(valid)\t|\tAcc: 98.2%(valid)\n",
      "Epoch: 15  | time in 0 minutes, 4 seconds\n",
      "\tLoss: 0.0001(train)\t|\tAcc: 100.0%(train)\n",
      "\tLoss: 0.0000(valid)\t|\tAcc: 98.2%(valid)\n",
      "Epoch: 16  | time in 0 minutes, 4 seconds\n",
      "\tLoss: 0.0001(train)\t|\tAcc: 100.0%(train)\n",
      "\tLoss: 0.0000(valid)\t|\tAcc: 98.3%(valid)\n",
      "Epoch: 17  | time in 0 minutes, 4 seconds\n",
      "\tLoss: 0.0001(train)\t|\tAcc: 100.0%(train)\n",
      "\tLoss: 0.0000(valid)\t|\tAcc: 98.2%(valid)\n",
      "Epoch: 18  | time in 0 minutes, 4 seconds\n",
      "\tLoss: 0.0001(train)\t|\tAcc: 100.0%(train)\n",
      "\tLoss: 0.0000(valid)\t|\tAcc: 98.2%(valid)\n",
      "Epoch: 19  | time in 0 minutes, 4 seconds\n",
      "\tLoss: 0.0001(train)\t|\tAcc: 100.0%(train)\n",
      "\tLoss: 0.0000(valid)\t|\tAcc: 98.2%(valid)\n",
      "Epoch: 20  | time in 0 minutes, 4 seconds\n",
      "\tLoss: 0.0001(train)\t|\tAcc: 100.0%(train)\n",
      "\tLoss: 0.0000(valid)\t|\tAcc: 98.3%(valid)\n",
      "Epoch: 21  | time in 0 minutes, 4 seconds\n",
      "\tLoss: 0.0001(train)\t|\tAcc: 100.0%(train)\n",
      "\tLoss: 0.0000(valid)\t|\tAcc: 98.3%(valid)\n",
      "Epoch: 22  | time in 0 minutes, 4 seconds\n",
      "\tLoss: 0.0001(train)\t|\tAcc: 100.0%(train)\n",
      "\tLoss: 0.0000(valid)\t|\tAcc: 98.2%(valid)\n",
      "Epoch: 23  | time in 0 minutes, 4 seconds\n",
      "\tLoss: 0.0001(train)\t|\tAcc: 100.0%(train)\n",
      "\tLoss: 0.0000(valid)\t|\tAcc: 98.3%(valid)\n",
      "Epoch: 24  | time in 0 minutes, 4 seconds\n",
      "\tLoss: 0.0001(train)\t|\tAcc: 100.0%(train)\n",
      "\tLoss: 0.0000(valid)\t|\tAcc: 98.3%(valid)\n",
      "Epoch: 25  | time in 0 minutes, 4 seconds\n",
      "\tLoss: 0.0001(train)\t|\tAcc: 100.0%(train)\n",
      "\tLoss: 0.0000(valid)\t|\tAcc: 98.2%(valid)\n",
      "Epoch: 26  | time in 0 minutes, 4 seconds\n",
      "\tLoss: 0.0001(train)\t|\tAcc: 100.0%(train)\n",
      "\tLoss: 0.0000(valid)\t|\tAcc: 98.2%(valid)\n",
      "Epoch: 27  | time in 0 minutes, 4 seconds\n",
      "\tLoss: 0.0001(train)\t|\tAcc: 100.0%(train)\n",
      "\tLoss: 0.0000(valid)\t|\tAcc: 98.3%(valid)\n",
      "Epoch: 28  | time in 0 minutes, 4 seconds\n",
      "\tLoss: 0.0001(train)\t|\tAcc: 100.0%(train)\n",
      "\tLoss: 0.0000(valid)\t|\tAcc: 98.2%(valid)\n",
      "Epoch: 29  | time in 0 minutes, 4 seconds\n",
      "\tLoss: 0.0001(train)\t|\tAcc: 100.0%(train)\n",
      "\tLoss: 0.0000(valid)\t|\tAcc: 98.3%(valid)\n",
      "Epoch: 30  | time in 0 minutes, 4 seconds\n",
      "\tLoss: 0.0001(train)\t|\tAcc: 100.0%(train)\n",
      "\tLoss: 0.0000(valid)\t|\tAcc: 98.2%(valid)\n",
      "Epoch: 31  | time in 0 minutes, 4 seconds\n",
      "\tLoss: 0.0001(train)\t|\tAcc: 100.0%(train)\n",
      "\tLoss: 0.0000(valid)\t|\tAcc: 98.2%(valid)\n",
      "Epoch: 32  | time in 0 minutes, 4 seconds\n",
      "\tLoss: 0.0001(train)\t|\tAcc: 100.0%(train)\n",
      "\tLoss: 0.0000(valid)\t|\tAcc: 98.2%(valid)\n",
      "Epoch: 33  | time in 0 minutes, 4 seconds\n",
      "\tLoss: 0.0001(train)\t|\tAcc: 100.0%(train)\n",
      "\tLoss: 0.0000(valid)\t|\tAcc: 98.3%(valid)\n",
      "Epoch: 34  | time in 0 minutes, 4 seconds\n",
      "\tLoss: 0.0001(train)\t|\tAcc: 100.0%(train)\n",
      "\tLoss: 0.0000(valid)\t|\tAcc: 98.2%(valid)\n",
      "Epoch: 35  | time in 0 minutes, 4 seconds\n",
      "\tLoss: 0.0001(train)\t|\tAcc: 100.0%(train)\n",
      "\tLoss: 0.0000(valid)\t|\tAcc: 98.2%(valid)\n",
      "Epoch: 36  | time in 0 minutes, 4 seconds\n",
      "\tLoss: 0.0001(train)\t|\tAcc: 100.0%(train)\n",
      "\tLoss: 0.0000(valid)\t|\tAcc: 98.2%(valid)\n",
      "Epoch: 37  | time in 0 minutes, 4 seconds\n",
      "\tLoss: 0.0001(train)\t|\tAcc: 100.0%(train)\n",
      "\tLoss: 0.0000(valid)\t|\tAcc: 98.2%(valid)\n",
      "Epoch: 38  | time in 0 minutes, 4 seconds\n",
      "\tLoss: 0.0001(train)\t|\tAcc: 100.0%(train)\n",
      "\tLoss: 0.0000(valid)\t|\tAcc: 98.3%(valid)\n",
      "Epoch: 39  | time in 0 minutes, 4 seconds\n",
      "\tLoss: 0.0001(train)\t|\tAcc: 100.0%(train)\n",
      "\tLoss: 0.0000(valid)\t|\tAcc: 98.3%(valid)\n",
      "Epoch: 40  | time in 0 minutes, 4 seconds\n",
      "\tLoss: 0.0001(train)\t|\tAcc: 100.0%(train)\n",
      "\tLoss: 0.0000(valid)\t|\tAcc: 98.2%(valid)\n",
      "Epoch: 41  | time in 0 minutes, 4 seconds\n",
      "\tLoss: 0.0001(train)\t|\tAcc: 100.0%(train)\n",
      "\tLoss: 0.0000(valid)\t|\tAcc: 98.3%(valid)\n",
      "Epoch: 42  | time in 0 minutes, 4 seconds\n",
      "\tLoss: 0.0001(train)\t|\tAcc: 100.0%(train)\n",
      "\tLoss: 0.0000(valid)\t|\tAcc: 98.3%(valid)\n",
      "Epoch: 43  | time in 0 minutes, 4 seconds\n",
      "\tLoss: 0.0001(train)\t|\tAcc: 100.0%(train)\n",
      "\tLoss: 0.0000(valid)\t|\tAcc: 98.2%(valid)\n",
      "Epoch: 44  | time in 0 minutes, 4 seconds\n",
      "\tLoss: 0.0001(train)\t|\tAcc: 100.0%(train)\n",
      "\tLoss: 0.0000(valid)\t|\tAcc: 98.2%(valid)\n",
      "Epoch: 45  | time in 0 minutes, 4 seconds\n",
      "\tLoss: 0.0001(train)\t|\tAcc: 100.0%(train)\n",
      "\tLoss: 0.0000(valid)\t|\tAcc: 98.2%(valid)\n",
      "Epoch: 46  | time in 0 minutes, 4 seconds\n",
      "\tLoss: 0.0001(train)\t|\tAcc: 100.0%(train)\n",
      "\tLoss: 0.0000(valid)\t|\tAcc: 98.2%(valid)\n",
      "Epoch: 47  | time in 0 minutes, 4 seconds\n",
      "\tLoss: 0.0001(train)\t|\tAcc: 100.0%(train)\n",
      "\tLoss: 0.0000(valid)\t|\tAcc: 98.2%(valid)\n",
      "Epoch: 48  | time in 0 minutes, 4 seconds\n",
      "\tLoss: 0.0001(train)\t|\tAcc: 100.0%(train)\n",
      "\tLoss: 0.0000(valid)\t|\tAcc: 98.2%(valid)\n",
      "Epoch: 49  | time in 0 minutes, 4 seconds\n",
      "\tLoss: 0.0001(train)\t|\tAcc: 100.0%(train)\n",
      "\tLoss: 0.0000(valid)\t|\tAcc: 98.2%(valid)\n",
      "Epoch: 50  | time in 0 minutes, 4 seconds\n",
      "\tLoss: 0.0001(train)\t|\tAcc: 100.0%(train)\n",
      "\tLoss: 0.0000(valid)\t|\tAcc: 98.2%(valid)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from torch.utils.data.dataset import random_split\n",
    "N_EPOCHS = 50\n",
    "min_valid_loss = float('inf')\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=4.0)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)\n",
    "\n",
    "train_len = int(len(train_dataset) * 0.95)\n",
    "sub_train_, sub_valid_ = \\\n",
    "    random_split(train_dataset, [train_len, len(train_dataset) - train_len])\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    train_loss, train_acc = train_func(sub_train_)\n",
    "    valid_loss, valid_acc = test(sub_valid_)\n",
    "\n",
    "    secs = int(time.time() - start_time)\n",
    "    mins = secs / 60\n",
    "    secs = secs % 60\n",
    "\n",
    "    print('Epoch: %d' %(epoch + 1), \" | time in %d minutes, %d seconds\" %(mins, secs))\n",
    "    print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.1f}%(train)')\n",
    "    print(f'\\tLoss: {valid_loss:.4f}(valid)\\t|\\tAcc: {valid_acc * 100:.1f}%(valid)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the results of test dataset...\n",
      "\tLoss: 0.0003(test)\t|\tAcc: 89.8%(test)\n"
     ]
    }
   ],
   "source": [
    "print('Checking the results of test dataset...')\n",
    "test_loss, test_acc = test(test_dataset)\n",
    "print(f'\\tLoss: {test_loss:.4f}(test)\\t|\\tAcc: {test_acc * 100:.1f}%(test)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a Sports news\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from torchtext.data.utils import ngrams_iterator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "ag_news_label = {1 : \"World\",\n",
    "                 2 : \"Sports\",\n",
    "                 3 : \"Business\",\n",
    "                 4 : \"Sci/Tec\"}\n",
    "\n",
    "def predict(text, model, vocab, ngrams):\n",
    "    tokenizer = get_tokenizer(\"basic_english\")\n",
    "    with torch.no_grad():\n",
    "        text = torch.tensor([vocab[token]\n",
    "                            for token in ngrams_iterator(tokenizer(text), ngrams)])\n",
    "        output = model(text, torch.tensor([0]))\n",
    "        return output.argmax(1).item() + 1\n",
    "\n",
    "ex_text_str = \"MEMPHIS, Tenn. – Four days ago, Jon Rahm was \\\n",
    "    enduring the season’s worst weather conditions on Sunday at The \\\n",
    "    Open on his way to a closing 75 at Royal Portrush, which \\\n",
    "    considering the wind and the rain was a respectable showing. \\\n",
    "    Thursday’s first round at the WGC-FedEx St. Jude Invitational \\\n",
    "    was another story. With temperatures in the mid-80s and hardly any \\\n",
    "    wind, the Spaniard was 13 strokes better in a flawless round. \\\n",
    "    Thanks to his best putting performance on the PGA Tour, Rahm \\\n",
    "    finished with an 8-under 62 for a three-stroke lead, which \\\n",
    "    was even more impressive considering he’d never played the \\\n",
    "    front nine at TPC Southwind.\"\n",
    "\n",
    "vocab = train_dataset.get_vocab()\n",
    "model = model.to(\"cpu\")\n",
    "\n",
    "print(\"This is a %s news\" %ag_news_label[predict(ex_text_str, model, vocab, 2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "doc_cls",
   "language": "python",
   "name": "doc_cls"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
