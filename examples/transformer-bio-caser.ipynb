{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-1-e2cf144c16b8>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0msys\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0msys\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'../../'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mutils\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpreprocess\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mPreProcess\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      4\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0msklearn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmodel_selection\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mtrain_test_split\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mhtml\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../../')\n",
    "from utils.preprocess import PreProcess\n",
    "from sklearn.model_selection import train_test_split\n",
    "import html\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectKBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_util import biocaser2text\n",
    "data_file = \"../../data/biocaster/BioCaster.3.xml\"\n",
    "data_df = biocaser2text(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df[\"Category\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_name = data_df.columns[0]\n",
    "    # print(column_name)\n",
    "pre_processor = PreProcess(data_df, column_name)\n",
    "# todo: change code to provide all functions in class definition.\n",
    "data = pre_processor.clean_html()\n",
    "data = pre_processor.remove_non_ascii()\n",
    "data = pre_processor.remove_spaces()\n",
    "data = pre_processor.remove_punctuation()\n",
    "data = pre_processor.stemming()\n",
    "data = pre_processor.lemmatization()\n",
    "data = pre_processor.stop_words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(min_df=1)\n",
    "data[\"tfidf\"] = list(vectorizer.fit_transform(data.Document.to_numpy()).toarray())\n",
    "len(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "data[\"bow\"] = list(vectorizer.fit_transform(data.Document.to_numpy()).toarray())\n",
    "len(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(ngram_range=(2,2),min_df=2)\n",
    "data[\"bigram\"] =list(vectorizer.fit_transform(data.Document.to_numpy()).toarray())\n",
    "len(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TriGram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(ngram_range=(3,3),min_df=2)\n",
    "data[\"trigram\"] = list(vectorizer.fit_transform(data.Document.to_numpy()).toarray())\n",
    "len(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import chi2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF + Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack\n",
    "X = hstack([data[\"tfidf\"],data[\"bow\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:20<00:00,  8.01s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bow 0.8955009900990099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:53<00:00,  5.31s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf 0.9002019801980199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:25<00:00,  8.51s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigram 0.8610178217821783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:54<00:00,  5.47s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bigram', 'trigram'] 0.861329702970297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:58<00:00,  5.90s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tfidf', 'bigram'] 0.9013831683168316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:06<00:00,  6.60s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bow', 'bigram'] 0.8938148514851486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:59<00:00,  5.97s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tfidf', 'trigram'] 0.9018009900990099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:05<00:00,  6.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bow', 'trigram'] 0.8930029702970298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "features = [\"bow\",\"tfidf\",\"bigram\",[\"bigram\",\"trigram\"],[\"tfidf\",\"bigram\"],[\"bow\",\"bigram\"],[\"tfidf\",\"trigram\"],[\"bow\",\"trigram\"]]\n",
    "for col in features:\n",
    "    acc = score_NB(data,col)\n",
    "    print(col,acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [07:27<00:00, 44.74s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bow 0.9011207920792079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [09:17<00:00, 55.74s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf 0.9234297029702973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [12:44<00:00, 76.48s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigram 0.8603009900990101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [11:48<00:00, 70.87s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bigram', 'trigram'] 0.8527415841584158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [11:50<00:00, 71.08s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tfidf', 'bigram'] 0.9123455445544556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [07:13<00:00, 43.34s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bow', 'bigram'] 0.9016861386138615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [11:49<00:00, 70.93s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tfidf', 'trigram'] 0.9130683168316832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [07:11<00:00, 43.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bow', 'trigram'] 0.9065821782178218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for col in features:\n",
    "    acc = score_svm(data,col)\n",
    "    print(col,acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:26<00:00,  8.68s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bow 0.8342168316831682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:03<00:00,  6.38s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf 0.8210178217821781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:15<00:00, 13.54s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigram 0.7832801980198021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:44<00:00, 10.46s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bigram', 'trigram'] 0.7816415841584159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:08<00:00,  6.89s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tfidf', 'bigram'] 0.8136752475247524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:11<00:00,  7.14s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bow', 'bigram'] 0.8245188118811881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:08<00:00,  6.83s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tfidf', 'trigram'] 0.817660396039604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:11<00:00,  7.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bow', 'trigram'] 0.8237217821782178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for col in features:\n",
    "    acc = score_id3(data,col)\n",
    "    print(col,acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_svm(data,col):\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    from sklearn import svm\n",
    "    from sklearn.utils import shuffle\n",
    "    clf = svm.SVC(kernel='linear', C=1)\n",
    "\n",
    "    scores_sum = []\n",
    "    for i in tqdm(range(10)):\n",
    "        data=shuffle(data)\n",
    "        if type(col)==list:\n",
    "            X = []\n",
    "            x1 = data[col[0]].to_numpy()\n",
    "            x2 = data[col[1]].to_numpy()\n",
    "            for f1,f2 in zip(x1,x2):\n",
    "                X.append(f1+f1)\n",
    "        else:\n",
    "            X = list(data[col])\n",
    "        ch2 = SelectKBest(chi2, k=9000)\n",
    "        step = [('sel', ch2), ('cls', clf)]\n",
    "        clf = Pipeline(step)\n",
    "        y = data.Category.to_numpy()\n",
    "        scores = cross_val_score(clf, X, y, cv=10, scoring='accuracy')\n",
    "        score_mean = scores.mean()\n",
    "        scores_sum.append(score_mean)\n",
    "    return sum(scores_sum) / len(scores_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [09:18<00:00, 55.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bow', 'trigram'] 0.9250297029702971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "acc = score_svm(data,\"tfidf\")\n",
    "print(col,acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_NB(data,col):\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    from sklearn import svm\n",
    "    from sklearn.utils import shuffle\n",
    "    from sklearn.naive_bayes import BernoulliNB\n",
    "    clf = BernoulliNB()\n",
    "\n",
    "    scores_sum = []\n",
    "    for i in tqdm(range(10)):\n",
    "        data=shuffle(data)\n",
    "        if type(col)==list:\n",
    "            X = []\n",
    "            x1 = data[col[0]].to_numpy()\n",
    "            x2 = data[col[1]].to_numpy()\n",
    "            for f1,f2 in zip(x1,x2):\n",
    "                X.append(f1+f1)\n",
    "        else:\n",
    "            X = list(data[col])\n",
    "        ch2 = SelectKBest(chi2, k=9000)\n",
    "        step = [('sel', ch2), ('cls', clf)]\n",
    "        clf = Pipeline(step)\n",
    "        y = data.Category\n",
    "        scores = cross_val_score(clf, X, y, cv=10, scoring='accuracy')\n",
    "        score_mean = scores.mean()\n",
    "        scores_sum.append(score_mean)\n",
    "    return sum(scores_sum) / len(scores_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DecisionTreeClassifier ID3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_id3(data,col):\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    from sklearn import svm\n",
    "    from sklearn.utils import shuffle\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    clf = DecisionTreeClassifier(random_state=0,criterion='entropy')\n",
    "\n",
    "    scores_sum = []\n",
    "    for i in tqdm(range(10)):\n",
    "        data=shuffle(data)\n",
    "        if type(col)==list:\n",
    "            X = []\n",
    "            x1 = data[col[0]].to_numpy()\n",
    "            x2 = data[col[1]].to_numpy()\n",
    "            for f1,f2 in zip(x1,x2):\n",
    "                X.append(f1+f1)\n",
    "        else:\n",
    "            X = list(data[col])\n",
    "        ch2 = SelectKBest(chi2, k=9000)\n",
    "        step = [('sel', ch2), ('cls', clf)]\n",
    "        clf = Pipeline(step)\n",
    "        y = data.Category\n",
    "        scores = cross_val_score(clf, X, y, cv=10, scoring='accuracy')\n",
    "        score_mean = scores.mean()\n",
    "        scores_sum.append(score_mean)\n",
    "    return sum(scores_sum) / len(scores_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_id3(data,'trigram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DecisionTreeClassifier C4.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_c45(data,col):\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    from sklearn import svm\n",
    "    from sklearn.utils import shuffle\n",
    "    from c45 import C45\n",
    "    model = C45()\n",
    "\n",
    "    scores_sum = []\n",
    "    for i in tqdm(range(10)):\n",
    "        data=shuffle(data)\n",
    "        X = list(data[col])\n",
    "        y = data.Category\n",
    "        scores = cross_val_score(model, X, y, cv=10, scoring='accuracy')\n",
    "        score_mean = scores.mean()\n",
    "        scores_sum.append(score_mean)\n",
    "    return sum(scores_sum) / len(scores_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_c45(data,'trigram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_process_data(data_df):\n",
    "    column_name = data_df.columns[0]\n",
    "    # print(column_name)\n",
    "    pre_processor = PreProcess(data_df, column_name)\n",
    "    # todo: change code to provide all functions in class definition.\n",
    "    data = pre_processor.clean_html()\n",
    "    data = pre_processor.remove_non_ascii()\n",
    "    data = pre_processor.remove_spaces()\n",
    "    data = pre_processor.remove_punctuation()\n",
    "    data = pre_processor.stemming()\n",
    "    data = pre_processor.lemmatization()\n",
    "    data = pre_processor.stop_words()\n",
    "    train_x, test_x, train_y, test_y = train_test_split(data.Document, data.Category, test_size=0.20)\n",
    "    tfidf_transformer = TfidfVectorizer(min_df=1)\n",
    "    train_vectors = tfidf_transformer.fit_transform(train_x)\n",
    "    return train_vectors, train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = read_process_data(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x.todense(),y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_ascii(df):\n",
    "    df = df.apply(lambda x: \"\".join(i for i in x if ord(i) < 128))\n",
    "    return df\n",
    "class PreProcess(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    This class contains all text pre-processing function\n",
    "    # Input parameters: Dataframe, Column_name on which function needs to be applied\n",
    "    # Output parameters: Return dataframe after applying operations\n",
    "    \"\"\"\n",
    "    # todo: Pass functions as a list of arguments to apply in the class\n",
    "    # todo: make set of words before applying all operations to reduce processing time.\n",
    "    def __init__(self):\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.lemmatiser = WordNetLemmatizer()\n",
    "        # pass\n",
    "\n",
    "    def clean_html(df):\n",
    "        \"\"\"remove html entities\"\"\"\n",
    "        df = df.apply(html.unescape)\n",
    "        return df\n",
    "\n",
    "    def remove_spaces(df):\n",
    "        df = df.apply(lambda x: x.replace('\\n', ' '))\n",
    "        df = df.apply(lambda x: x.replace('\\t', ' '))\n",
    "        df = df.apply(lambda x: x.replace('  ', ' '))\n",
    "        df = df.apply(lambda x: x.lower())\n",
    "        return df\n",
    "\n",
    "    def remove_punctuation(self):\n",
    "        tr = str.maketrans(\"\", \"\", string.punctuation)\n",
    "        # self.data[self.column_name] = self.data[self.column_name].apply(lambda x: \" \".join([item.translate(tr)\n",
    "        #                                                                 for item in x.split()]))\n",
    "        self.data[self.column_name] = self.data[self.column_name].apply(lambda x: x.translate(tr))\n",
    "        return self.data\n",
    "\n",
    "    def stemming(self):\n",
    "        # todo: provide option of selecting stemmer.\n",
    "        snowball_stemmer = SnowballStemmer('english')\n",
    "        # self.data[self.column_name] = self.data[self.column_name].apply(lambda x: \" \".join([snowball_stemmer.stem(item)\n",
    "        #                                                                 for item in x.split()]))\n",
    "        self.data[self.column_name] = self.data[self.column_name].apply(lambda x: \" \".join([self.stemmer.stem(item)\n",
    "                                                                        for item in x.split()]))\n",
    "        return self.data\n",
    "\n",
    "    def lemmatization(self):\n",
    "        self.data[self.column_name] = self.data[self.column_name].apply(lambda x: \" \".join([self.lemmatiser.lemmatize(item)\n",
    "                                                                        for item in x.split()]))\n",
    "        return self.data\n",
    "\n",
    "    def stop_words(self):\n",
    "        stop = stopwords.words('english')\n",
    "        self.data[self.column_name] = self.data[self.column_name].apply(lambda x: \" \".join(set([item for item in x.split() if\n",
    "                                                                                       item not in stop])))\n",
    "        return self.data\n",
    "    \n",
    "    def transform(self, df):\n",
    "        df = remove_non_ascii(df)\n",
    "        df = clean_html(df)\n",
    "        return df\n",
    "        \n",
    "    def fit(self, df, y=None, **fit_params):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class PreProcessing(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Custom Pre-Processing estimator for our use-case\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def transform(self, df):\n",
    "        \"\"\"\n",
    "        Preprocessing steps for text processing\n",
    "        \"\"\"\n",
    "#         columns = df.columns()\n",
    "        df = df.apply(lambda x: x.lower())\n",
    "        df = df.apply(lambda x: \"\".join(i for i in x if ord(i) < 128))\n",
    "        df = df.apply(html.unescape)\n",
    "        return df\n",
    "\n",
    "    def fit(self, df, y=None, **fit_params):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('Test1', PreProcess())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx =pipe.fit_transform(df.Document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = [{'A': [1,2,3,4], 'B': [2,3,4,5], 'C': [4,5,5,6], 'D': [6,3,4,5]}, {'A': [2,3,5,6], 'B': [3,4,6,6], 'C': [3,4,5,7], 'D': [2,6,3,4]}, {'A': [8,9,6,7], 'B': [5,7,9,5], 'C': [3,7,9,5], 'D': [5,7,9,8]}]\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"E\"]=np.array([[1,2,3,4],  [2,3,4,5], [4,5,5,6]])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mmll0537\r\n"
     ]
    }
   ],
   "source": [
    "!hostname"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}