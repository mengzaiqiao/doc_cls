{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load BioCaser Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get promed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get promed-extended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 3430 files\n",
      "Remain 3377 files after filter_out with alerting\n",
      "found files: 3377\n",
      "found 3430 files\n",
      "Remain 53 files after filter_in with alerting\n",
      "found files: 53\n",
      "found 3862 files\n",
      "found files: 3862\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docs</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n \\n \\n \\n AVIAN INFLUENZA, HUMAN (109) - IND...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Health officials in Hong Kong say that...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;h&gt; Coronavirus Detected In Patient Quarantine...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\n \\n EQUINE INFLUENZA - CHINA\\n \\n **********...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n \\n \\n \\n SALMONELLOSIS SENFTENBERG, BASIL -...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                docs  labels\n",
       "0  \\n \\n \\n \\n AVIAN INFLUENZA, HUMAN (109) - IND...       1\n",
       "1          Health officials in Hong Kong say that...       1\n",
       "2  <h> Coronavirus Detected In Patient Quarantine...       1\n",
       "3  \\n \\n EQUINE INFLUENZA - CHINA\\n \\n **********...       1\n",
       "4  \\n \\n \\n \\n SALMONELLOSIS SENFTENBERG, BASIL -...       1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.data_util import (\n",
    "    biocaser2text,\n",
    "    get_raw_extended_promed_df,\n",
    "    get_raw_promed_df,\n",
    ")\n",
    "data_df = get_raw_extended_promed_df()\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model = gensim.models.KeyedVectors.load_word2vec_format('/home/zm324/workspace/doc_cls/resources/word2vec/GoogleNews-vectors-negative300.bin.gz', binary=True) \n",
    "# word2vec = model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import gensim\n",
    "# import time\n",
    "# import numpy as np\n",
    "# from tqdm import tqdm\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# from torch.utils.data import DataLoader\n",
    "# embedding = nn.EmbeddingBag(1, 300, sparse=True)\n",
    "# embedding.from_pretrained(torch.tensor([word2vec[\"word\"]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gensim\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class TextLinear(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class,emb_pretrain):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "        self.pre_train = emb_pretrain\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.from_pretrained(self.pre_train)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        return self.fc(embedded)\n",
    "def generate_batch_wo_label(batch):\n",
    "    text = [torch.tensor(entry) for entry in batch]\n",
    "    offsets = [0] + [len(entry) for entry in text]\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text = torch.cat(text)\n",
    "    return text, offsets\n",
    "\n",
    "def generate_batch(batch):\n",
    "    label = torch.tensor([entry[0] for entry in batch])\n",
    "    text = [torch.tensor(entry[1]) for entry in batch]\n",
    "    offsets = [0] + [len(entry) for entry in text]\n",
    "    # torch.Tensor.cumsum returns the cumulative sum\n",
    "    # of elements in the dimension dim.\n",
    "    # torch.Tensor([1.0, 2.0, 3.0]).cumsum(dim=0)\n",
    "\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text = torch.cat(text)\n",
    "    return text, offsets, label\n",
    "    \n",
    "class RandomWord2vec(object):\n",
    "    \n",
    "    def __init__(self, data_df, emb_dim=300, batch_size=128, max_epochs = 50, num_class = 2):\n",
    "        self.device = 'cuda:1' if torch.cuda.is_available() else \"cpu\"\n",
    "        from utils.preprocess import PreProcess\n",
    "        pre_processor = PreProcess(data_df, \"docs\",lower=False)\n",
    "        # todo: change code to provide all functions in class definition.\n",
    "        pre_processor.clean_html()\n",
    "        pre_processor.remove_non_ascii()\n",
    "        pre_processor.remove_spaces()\n",
    "        pre_processor.remove_punctuation()\n",
    "        pre_processor.stop_words()\n",
    "        # pre_processor.tokenize()\n",
    "        data_df.head()\n",
    "        self.data_df = data_df\n",
    "        self.emb_dim = emb_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.max_epochs = max_epochs\n",
    "        self.num_class = num_class\n",
    "        self.word2vec = gensim.models.KeyedVectors.load_word2vec_format('/home/zm324/workspace/doc_cls/resources/word2vec/GoogleNews-vectors-negative300.bin.gz', binary=True).wv\n",
    "        self.build_vocab()\n",
    "        self.pre_train = torch.tensor(self.pre_train).to(self.device)\n",
    "        self.data_df.docs=self.data_df.docs.apply(self.doc2idx)\n",
    "        \n",
    "    def doc2idx(self, doc):\n",
    "        idxs = []\n",
    "        if type(doc) is not list:\n",
    "            doc = doc.split()\n",
    "        for word in doc:\n",
    "            if word in self.word_to_ix:\n",
    "                idxs.append(self.word_to_ix[word])\n",
    "        return idxs\n",
    "        \n",
    "    def build_vocab(self):\n",
    "        self.pre_train = []\n",
    "        word_to_ix = {}\n",
    "        for idx,row in self.data_df.iterrows():\n",
    "            doc = row.docs\n",
    "            if type(doc) is not list:\n",
    "                doc = doc.split()\n",
    "            for word in doc:\n",
    "                if word not in word_to_ix and word in self.word2vec:\n",
    "                    word_to_ix[word] = len(word_to_ix)\n",
    "                    self.pre_train.append(self.word2vec[word])\n",
    "        self.word_to_ix = word_to_ix\n",
    "        self.vocab_size = len(self.word_to_ix)\n",
    "    \n",
    "    \n",
    "    def train_epoch(self,X,y):\n",
    "\n",
    "        # Train the model\n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        tran_data = np.stack((y,X), axis=-1)\n",
    "        data = DataLoader(tran_data, batch_size=self.batch_size, shuffle=True,\n",
    "                          collate_fn=generate_batch)\n",
    "        for i, (text, offsets, cls) in enumerate(data):\n",
    "            self.optimizer.zero_grad()\n",
    "            text, offsets, cls = text.to(self.device), offsets.to(self.device), cls.to(self.device)\n",
    "            output = self.model(text, offsets)\n",
    "            loss = self.criterion(output, cls)\n",
    "            train_loss += loss.item()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            train_acc += (output.argmax(1) == cls).sum().item()\n",
    "\n",
    "        # Adjust the learning rate\n",
    "        self.scheduler.step()\n",
    "\n",
    "        return train_loss / len(X), train_acc / len(X)\n",
    "    \n",
    "    def fit(self,X,y):\n",
    "        self.model = TextLinear(self.vocab_size, self.emb_dim, self.num_class, self.pre_train).to(self.device)\n",
    "        min_valid_loss = float('inf')\n",
    "        self.criterion = torch.nn.CrossEntropyLoss().to(self.device)\n",
    "        self.optimizer = torch.optim.SGD(self.model.parameters(), lr=4.0)\n",
    "        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, 1, gamma=0.9)\n",
    "\n",
    "        for epoch in tqdm(range(self.max_epochs)):\n",
    "            start_time = time.time()\n",
    "            train_loss, train_acc = self.train_epoch(X,y)\n",
    "            secs = int(time.time() - start_time)\n",
    "            mins = secs / 60\n",
    "            secs = secs % 60\n",
    "\n",
    "#             print('Epoch: %d' %(epoch + 1), \" | time in %d minutes, %d seconds\" %(mins, secs))\n",
    "#             print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.1f}%(train)')\n",
    "\n",
    "    def predict(self,X):\n",
    "        loss = 0\n",
    "        acc = 0\n",
    "        data = DataLoader(X, batch_size=self.batch_size, collate_fn=generate_batch_wo_label)\n",
    "        pred_y = []\n",
    "        for text, offsets in data:\n",
    "            text, offsets = text.to(self.device), offsets.to(self.device)\n",
    "            with torch.no_grad():\n",
    "                output = self.model(text, offsets).to('cpu')\n",
    "                pred_y+=list(output.argmax(1))\n",
    "        return pred_y\n",
    "    #     return loss / len(data_), acc / len(data_)\n",
    "\n",
    "    def test(self,data_):\n",
    "        loss = 0\n",
    "        acc = 0\n",
    "        data = DataLoader(data_, batch_size=self.batch_size, collate_fn=generate_batch)\n",
    "        pred_y = []\n",
    "        for text, offsets, cls in data:\n",
    "            text, offsets, cls = text.to(self.device), offsets.to(self.device), cls.to(self.device)\n",
    "            with torch.no_grad():\n",
    "                output = self.model(text, offsets)\n",
    "                loss = criterion(output, cls)\n",
    "                loss += loss.item()\n",
    "                pred_y = output.argmax(1)\n",
    "                acc += (output.argmax(1) == cls).sum().item()\n",
    "    #     return pred_y\n",
    "        return loss / len(data_), acc / len(data_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-f68dcc3737dd>:65: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  self.word2vec = gensim.models.KeyedVectors.load_word2vec_format('/home/zm324/workspace/doc_cls/resources/word2vec/GoogleNews-vectors-negative300.bin.gz', binary=True).wv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docs</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 5...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[165, 166, 167, 168, 169, 170, 47, 171, 164, 1...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[327, 328, 329, 330, 331, 332, 333, 334, 15, 3...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[447, 48, 448, 449, 450, 451, 452, 453, 51, 45...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7287</th>\n",
       "      <td>[277, 129, 1128, 3370, 84550, 5604, 3087, 5739...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7288</th>\n",
       "      <td>[902, 49, 1306, 1713, 12, 752, 756, 6860, 759,...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7289</th>\n",
       "      <td>[5688, 14500, 14800, 6139, 5518, 81178, 756, 9...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7290</th>\n",
       "      <td>[2292, 56543, 742, 1277, 451, 7516, 752, 24535...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7291</th>\n",
       "      <td>[26543, 12668, 6388, 902, 87287, 9149, 6390, 2...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7292 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   docs  labels\n",
       "0     [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...       1\n",
       "1     [46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 5...       1\n",
       "2     [165, 166, 167, 168, 169, 170, 47, 171, 164, 1...       1\n",
       "3     [327, 328, 329, 330, 331, 332, 333, 334, 15, 3...       1\n",
       "4     [447, 48, 448, 449, 450, 451, 452, 453, 51, 45...       1\n",
       "...                                                 ...     ...\n",
       "7287  [277, 129, 1128, 3370, 84550, 5604, 3087, 5739...       2\n",
       "7288  [902, 49, 1306, 1713, 12, 752, 756, 6860, 759,...       2\n",
       "7289  [5688, 14500, 14800, 6139, 5518, 81178, 756, 9...       2\n",
       "7290  [2292, 56543, 742, 1277, 451, 7516, 752, 24535...       2\n",
       "7291  [26543, 12668, 6388, 902, 87287, 9149, 6390, 2...       2\n",
       "\n",
       "[7292 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RandomWord2vec(data_df)\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]/home/zm324/anaconda3/envs/doc_cls/lib/python3.8/site-packages/numpy/core/_asarray.py:136: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order, subok=True)\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-59e644a86b12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"docs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"labels\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"docs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-f68dcc3737dd>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m             \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m             \u001b[0msecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0mmins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msecs\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m60\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-f68dcc3737dd>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffsets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RepeatedKFold\n",
    "from metrics import Accuracy,Precision,Recall,F1Score\n",
    "metrics = [Accuracy(),Precision(),Recall(),F1Score()]\n",
    "\n",
    "avg_results = {m.name:[] for m in metrics}\n",
    "rkf = RepeatedKFold(n_splits=10, n_repeats=10, random_state=2652124)\n",
    "for train_index, test_index in rkf.split(data_df):\n",
    "    train_set = data_df.iloc[train_index]\n",
    "    test_set = data_df.iloc[test_index]\n",
    "    X = list(train_set[\"docs\"])\n",
    "    y_true = list(train_set[\"labels\"])\n",
    "    model.fit(X,y_true)\n",
    "\n",
    "    X = list(test_set[\"docs\"])\n",
    "    y_true = list(test_set[\"labels\"])\n",
    "    y_pred = model.predict(X)\n",
    "    for mt in metrics:\n",
    "        avg_results[mt.name].append(mt.compute(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mt in metrics:\n",
    "    avg_results[mt.name]=np.mean(avg_results[mt.name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "doc_cls",
   "language": "python",
   "name": "doc_cls"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
