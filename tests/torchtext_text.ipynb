{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [00:02<00:00, 9406.67lines/s]\n"
     ]
    }
   ],
   "source": [
    "from beta_nlp.data.cls_dataset import IMDB\n",
    "# obtain data and vocab with a custom tokenizer\n",
    "train_dataset, test_dataset = IMDB()\n",
    "vocab = train_dataset.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0),\n",
       " tensor([   13,   125,  1051,     5,   246,  1652,     8,   277,    66,    20,\n",
       "             6,   179,     3,  1051, 80455,    31,   619, 96978,     4, 13792,\n",
       "             5,  7074,     3,    13,   780,     8,    45,    14,     4,    13,\n",
       "            72,   126,     4,    22,    11,    10,     8,    57,   261,  1051,\n",
       "            18, 14852,   573,    10,     8,   338,  2014,    25,     2,   205,\n",
       "            24,     3,   694, 19018,     4,   705,  3425,   717,     4,  4548,\n",
       "          3961,     4,  4521,    15,   159,     9,    27,  1004,     2,   952,\n",
       "             4,     5,  2112,  4423,   111,   559,    34,  3014,    20,     6,\n",
       "             9,  1051,     9,   949,     3,    25,    13,     9,   152,   254,\n",
       "            46,    31,   154,     7,    26,    53,    46,    42,   110, 14852,\n",
       "           573,    10,    57,  1051,   261,     3,    11,     9,    16,    29,\n",
       "             3,    11,     9,    16,  2667,     5,  8688,     3,    24,   143,\n",
       "           185,   767,   238,    45,  1391,     5,   112,   917,     4,  1051,\n",
       "            10,     6,   524,    15,   130,    29,   199,   409,   607,    25,\n",
       "         25290,     3,   338,  2014,    24,     3,    11,   204,  1649,   678,\n",
       "          1304,     4,   252,    29,    18,     6,   620,  4104,     3,    11,\n",
       "             9,    16,    72,   864,     8,   462,    50,     2,   111,   132,\n",
       "            18,    39,    31,    29,   331,  6348,     4,    49,   983,     6,\n",
       "          5195,     7,   127,     3,    73,  1682,     5,  3316,    31,  1618,\n",
       "             5,   714,     4,   399,  1313,     8,   113,     3,     2,  1363,\n",
       "             7,   697,   129,    11,     9,    16,  1874,    18,    39,    33,\n",
       "             8,   215,   141,  1864, 14177,     9,    16,   697,     3,     3,\n",
       "             3,   886,    85,    67,    29,  1695,   155,     3, 14177,     9,\n",
       "            16, 11857,   227,    34,  1585,    12,    73, 15790,    18,    14,\n",
       "           743,     4,   705,     4,   870,  2100,    25,   155,    11,   211,\n",
       "         16053,  1970,    72,   928,    14,   351,    24, 39737,     0,     7,\n",
       "             6,   128, 37380,    90,   840,     3,  1700,     3,    44,     4,\n",
       "           510,   139,     6,   298,   112,     3,     5,   101,   703,    96,\n",
       "           153,    18,   164,   285,     3,     0,    36,  7762,    37,   137,\n",
       "           180,     3]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100684"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import datasets\n",
    "from torchtext.experimental.datasets import IMDB\n",
    "\n",
    "# set up tokenizer (the default on is basic_english tokenizer)\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "tokenizer = get_tokenizer(\"spacy\")\n",
    "\n",
    "# obtain data and vocab with a custom tokenizer\n",
    "train_dataset, test_dataset = IMDB(tokenizer=tokenizer)\n",
    "vocab = train_dataset.get_vocab()\n",
    "\n",
    "# use the default tokenizer\n",
    "train_dataset, test_dataset = IMDB()\n",
    "vocab = train_dataset.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7111)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "loss = F.binary_cross_entropy_with_logits(torch.tensor([1.,2.,0.]), torch.tensor([0.0,1,0.0]))\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "doc_cls",
   "language": "python",
   "name": "doc_cls"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
